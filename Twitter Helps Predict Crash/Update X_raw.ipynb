{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read word bags, 1000 words totally\n",
    "fr = open('selected_wordbag.txt','rb')  \n",
    "selected_wordbag = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read previous X_raw\n",
    "X_raw=pd.read_csv(\"X_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell only FOR THE FIRST TIME!!!\n",
    "#because nltk needs to download some dictionary\n",
    "test=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hillari'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this cell only FOR THE FIRST TIME!!!\n",
    "#because nltk needs to download some dictionary\n",
    "#this cell can be used to test the results of a word after Porter Algo\n",
    "porter_stemmer = PorterStemmer()\n",
    "porter_stemmer.stem(\"hillary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only use this function if your tweets is from 2015/1/1 to 2018/4/25\n",
    "\n",
    "#input:X_raw,previous dataframe; tweet_dfï¼Œdataframe of your new tweets\n",
    "\n",
    "def update_X(X_raw,tweet_df,pattern=re.compile(r'[a-zA-Z]{2,}'),stopwords=set(nltk.corpus.stopwords.words('english')),\n",
    "             selected_words=set(selected_wordbag)):\n",
    "    \n",
    "    N=tweet_df.shape[0]\n",
    "    \n",
    "    for i in range(N):\n",
    "        #test if the tweet is after 2015\n",
    "        if tweet_df.loc[i,'created_y']<2015:\n",
    "            break\n",
    "        \n",
    "        #test if the tweet has valid month, date and year\n",
    "        if np.isnan(tweet_df.loc[i,'created_y']) or np.isnan(tweet_df.loc[i,'created_m']) or np.isnan(tweet_df.loc[i,'created_d']):\n",
    "            continue\n",
    "        \n",
    "        if int(tweet_df.loc[i,'created_d'])==0:\n",
    "            continue\n",
    "        \n",
    "        #determine the index of the tweet based on its date\n",
    "        ind=(datetime(int(tweet_df.loc[i,'created_y']),int(tweet_df.loc[i,'created_m']),int(tweet_df.loc[i,'created_d']))\n",
    "             -datetime(2015,1,1)).days\n",
    "        \n",
    "        #count the words\n",
    "        mystring=tweet_df.loc[i,'text']\n",
    "        \n",
    "        #find all words with more than 2 characters\n",
    "        mylist=pattern.findall(mystring)\n",
    "        \n",
    "        #filter using stopwords\n",
    "        for j in range(len(mylist)):\n",
    "            mylist[j]=mylist[j].lower()\n",
    "        new_bag=[word for word in mylist if word not in stopwords]\n",
    "        \n",
    "        #do stemming\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        for j in range(len(new_bag)):\n",
    "            new_bag[j]=porter_stemmer.stem(new_bag[j])\n",
    "        \n",
    "        #update our X_raw\n",
    "        for j in new_bag:\n",
    "            if j in selected_words:\n",
    "                X_raw.loc[ind,j]+=1\n",
    "    \n",
    "    print(\"finished updating!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsDict={\n",
    "    'CNBC':'CNBC',\n",
    "    'StockTwits':'StockTwits',\n",
    "    'WSJ':'WSJMarkets',\n",
    "    'Benzinga.com':'Benzinga',\n",
    "}\n",
    "peopleDict={\n",
    "    'Carl Icahn':'Carl_C_Icahn',\n",
    "    'John Carney':'carney',\n",
    "    'Stephanie Link':'Stephanie_Link',\n",
    "    'Jim Cramer':'jimcramer',\n",
    "    'Jeffrey Gundlach':'TruthGundlach',\n",
    "    'Mohamed A. El-Erian':'elerianm',\n",
    "    'Joseph A. LaVorgna':'Lavorgnanomics',\n",
    "}\n",
    "instituteDict={\n",
    "    'Federal Reserve Bank of St.Louis':'stlouisfed',\n",
    "    'Bespoke Investment Group':'bespokeinvest',\n",
    "    'Morgan Stanley':'MorganStanley',\n",
    "    'Muddy Water Research':'muddywatersre',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n"
     ]
    }
   ],
   "source": [
    "for i in newsDict.values():\n",
    "    #This shows the advantage of our naming rules!\n",
    "    temp_data=pd.read_csv(i+\"_username.csv\")\n",
    "    update_X(X_raw,temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n"
     ]
    }
   ],
   "source": [
    "for i in peopleDict.values():\n",
    "    #This shows the advantage of our naming rules!\n",
    "    temp_data=pd.read_csv(i+\"_username.csv\")\n",
    "    update_X(X_raw,temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to save X_raw from time to time, and in other places, to avoid potential bugs \n",
    "X_raw.to_csv(\"X_raw.csv\",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n",
      "finished updating!\n"
     ]
    }
   ],
   "source": [
    "for i in instituteDict.values():\n",
    "    #This shows the advantage of our naming rules!\n",
    "    temp_data=pd.read_csv(i+\"_username.csv\")\n",
    "    update_X(X_raw,temp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw.to_csv(\"X_raw.csv\",encoding=\"utf-8\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
